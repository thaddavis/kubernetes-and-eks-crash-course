# STEPS

## create our cluster

- $ aws eks create-cluster \
--name getting-started-eks \
--role-arn $role_arn \
--resources-vpc-config subnetIds=<COMMA_SEPARATED_SUBNET_IDS>,securityGroupIds=<COMMAND_SEPARATED_SECURITY_GROUPS>,endpointPublicAccess=true,endpointPrivateAccess=false

ie:

- $ aws eks create-cluster \
--name getting-started-eks \
--role-arn $role_arn \
--resources-vpc-config subnetIds=subnet-0ec824f83ac1d024b,subnet-0c04b58c562493a95,subnet-02545f545bdd274b4,securityGroupIds=sg-0092b63fb31d0f534,endpointPublicAccess=true,endpointPrivateAccess=false \
--profile eks_user

## I was having IAM issues but used the following link to guide me in what permissions I needed to provision an EKS cluster

`https://eksctl.io/usage/minimum-iam-policies/`

I created an IAM user called `eks_user` and attached the policies in the ./policies folder and was able to create an EKS cluster

## verify the EKS cluster is created

`https://us-east-1.console.aws.amazon.com/eks/home?region=us-east-1#/clusters`

$ aws eks list-clusters

$ aws eks describe-cluster --name getting-started-eks

## Update local kubectl to point to cluster

$ aws eks update-kubeconfig --name getting-started-eks --region us-east-1

- I ran into bug that required me to update my aws-cli version to `2.7.24`

$ kubectl version
$ kubectl get nodes
$ kubectl config current-context
